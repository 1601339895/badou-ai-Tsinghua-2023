### 深度学习与神经网络
1. 深度学习
    1. 概论
        1. 机器学习是人工智能的核心，研究如何使用机器来模拟人类学习活动的一门学科。
        2. 深度学习（多层人工神经网络）是机器学习的分支，对数据进行表征学习的方法。
        3. 人工智能关系圈：以下3个，前面的包含后面的
            1. 机器学习
                1. 种实现人工智能的方法
                2. 使用归纳、综合，而不是演绎
                3. 分类
                    1. 传统机器学习算法：分类、聚类等等很多
                    2. 深度学习算法
            2. 深度学习：一种实现机器学习的技术
            3. 人工神经网络：：一种机器学习的算法。
    2. 神经网络
        1. 人是怎么思考的？--生物神经网络
        2. 人工神经网络分为两个阶段：
            1. 预激活阶段：接受来自其他n个神经元传递过来的信号，这些输入信号通过与相应的权重进行加权求和传递给下个阶段
            2. 把预激活的加权结果传递给激活函数
        3. 与生物神经网络的类比：
            1. 细胞核 - 神经元
            2. 树突 - 输入
            3. 轴突 - 输出
            4. 突触 - 权重
        4. 机器是怎么思考的？--人工智能网络
        5. 神经网络组成
            1. 神经元
                1. 模仿人体的神经元
                2. 公式：sum = WX + b
                    1. W、X都是矩阵
                    2. W代表权重，X是输入，WX代表权重累加过程
                    3. b是偏移量矩阵
                3. 重要的2个部分：
                    1. 输入：是特征向量。特征向量代表的是变化的方向。或者说，是最能代表这个事物的特征的方向。
                        1. 卷积提取的特征，就可以作为神经元的输入，否则全部像素点作为输入计算量太多了
                    2. 权重（权值）：就是特征值。有正有负，加强或抑制，同特征值一样。权重的绝对值大小，代表了输入信号对神经元的影响的大小。
                4. 神经元本质：一刀切，非黑即白
                    1. 神经元就是当h大于0时输出1，h小于0时输出0这么一个模型，它的实质就是把特征空间一切两半，认为两半分别属于两个类。
                5. 单个神经元的缺点：只能一刀切(怎么切取决于w和b)
                6. 解决方法：多层神经网络
            2. 神经网络
                1. 简介：
                    1. 神经网络是一种运算模型，由大量的节点（神经元）和之间相互的联接构成
                    2. 每两个节点间的联接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。
                    3. 网络的输出则依网络的连接方式、权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。
                    4. 单层神经网络（感知器）
                2. 多层神经网络
                    1. 神经网络是由多个神经元组合而成，前一个神经元的结果作为后一个神经元的输入，神经网络一般分为三层，第一层作为输入层，最后一层作为输出层，中间的全部是隐含层。依次组合而成。
                        1. 同层节点没有连接
                        2. 输入层可以接收图片的像素点
                        3. 输出层的节点个数和需求相符，如使用one hot编码方式
                    2. 理论证明，任何多层网络可以用三层网络近似地表示。
                    3. 一般凭经验来确定隐藏层到底应该有多少个节点，在测试的过程中也可以不断调整节点数以取得最佳效果。
                        1. 隐藏层的个数、层数都与输入和输出无关
                        2. 隐藏层的节点个数和隐藏层的层数的调整，要考虑算力
                3. 前馈神经网络
                    1. 人工神经网络模型主要考虑网络链接的拓扑结构、神经元特征、学习规则等。
                    2. 其中，前馈神经网络也称为多层感知机
                    3. 从输入到输出的一个正向、单向的过程
                    4. 结构
                        1. 输入层--固定的一层
                        2. 隐藏层--不固定的N层，参数调优调的就是隐藏层
                        3. 输出层--固定的一层
                4. 激活函数
                    1. 地位：激活函数是神经网络设计的一个核心单元。
                    2. 状态：
                        1. 激活
                        2. 抑制
                    3. 作用：为了在神经网络中引入非线性的学习和处理能力。
                    4. 激活函数需满足的条件：
                        1. 非线性
                            1. 为什么要把直线变成曲线？--数据分布没有那么的规则，用一条直线分不开，会漏掉几个点，用一条曲线就可以分开
                            2. 如果wx+b是一把直的刀，那么激活函数是一把曲线的刀
                        2. 可微性：可导可微
                        3. 单调性
                    5. 常用激活函数：（函数图像类似矩形脉冲）
                        1. sigmoid函数，过点(0, 0.5)，在0 到 1之间
                            1. 缺点：
                                1. 梯度饱和，看图可知，两边数值的梯度都为0；
                                2. 结果的平均值不为0，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。
                                3. 梯度弥散
                            2. 优点
                                1. 计算复杂度低，推理时间短
                        2. tanh函数，过点(0, 0.)，在-1 到 1之间
                            1. tanh(x) = 2σ(2x) - 1
                            2. 与σ对比：我们更追求推理时间更短
                                1. 训练时间：sigmoid > tanh
                                2. 推理时间：tanh > sigmoid
                            3. 缺点：
                                1. 梯度弥散没解决
                            4. 优点：
                                1. 解决了原点对称问题
                                2. 训练阶段比sigmoid更快
                        3. ReLU函数 f(x) = max (0, x)
                            1. 每一段是线性，但整体横折横折就是非线性的了
                            2. 最受欢迎的激活函数
                                1. 模拟人脑神经元的激活模型
                                2. 简单粗暴
                            3. 缺点
                                1. 梯度弥散没完全解决，在(-)部分相当于神经元死亡且不会复活
                            2. 优点：
                                1. 解决了部分梯度弥散问题
                                2. 收敛速度更快
                5. 神经元稀疏
                    1. 稀疏的原因：
                        1. ReLU 把所有的负值都变为0，而正值不变，这种操作被称为单侧抑制。正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。
                        2. 当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。
                        3. 为什么要做这种稀疏？
                            1. 模拟人脑神经元工作方式，人类大脑神经元也是有稀疏方式的
                            2. 大脑在思考时，同时被激活的神经元占总数的 1% - 4%，其他神经元处于被抑制状态
                            3. 减少计算量
                6. 张量（tensor）
                    1. tensor是一种数据结构
                    2. 张量一大特征是维度，在Python中，一个张量的维度可以通过读取它的.ndim属性来获取，常见的模型张量：(n, h, w, c)
                        1. 一个0维张量就是一个常量
                        2. 数组就等价与一维张量
                        3. 一个二维数组就是一个二维张量
                        4. 所谓n维张量，其实就是一维数组，数组中的每个元素都是n-1维张量（即多维数组的嵌套）
                            1. 由此可见，3维张量其实就是一个一维数组，数组中的每个元素就是2维数组。
                    3. 一个n维张量经常用一组数据来表示，它可以用(3,2,2)这组数据结合来表示，一个张量是几维度，那么括号里面就有几个数字。
                        1. (3,2,2)的意义：
                            1. 最外层是3个
                            2. 2：再里面一层有两个二位张量
                            3. 2：二维张量中含有两个常量(1维张量)
            3. 设计神经网络
                1. 概论：
                    1. 使用神经网络训练数据之前，必须先确定
                        1. 神经网络的层数
                            1. 输入、输出层都是固定的1层，单元个数也相对好确定，即特征数、像素点数等
                            2. 隐藏层相对不好确定，没有公式，可以根据实验测试和误差以及精确准确度来实验并改进
                        2. 每层的单元个数
                    2. 特征向量在被传入输入层时通常要先标准化到0-1之间，为了加速学习过程
                    3. 离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值，如one hot编码
                    4. 神经网络可以用来做分类问题，也可以解决回归问题
                2. 对隐藏层的感性认识
                    1. 拆解成一个个针对特征值的小问题
                    2. 每一个小问题还可以继续拆成更小的问题
                    3. 直到每一个问题可以用一个单独的神经元被回答
                    4. 隐藏层数和节点数越多判断的准确性，但也要考虑算力和存储，trade off
                    5. 先用卷积提取特征后才给隐藏层
                    6. 两个关键点：
                        1. 不能一味的增加隐藏层和节点个数
                        2. 隐藏层判断用的 w 是计算机自己算出来的，不能人为指定，人类只能指定最后的那个大问题
            4. 深度学习
                1. 什么是深度学习？
                    1. 深度神经网络 & 深度学习
                        1. 
